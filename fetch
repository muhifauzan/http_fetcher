#!/usr/bin/env python3

from argparse import ArgumentParser
from bs4 import BeautifulSoup as XMLParser
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse

import logging as lg
import re
import requests as rq


def get_html_text(session, url):
    try:
        response = session.get(url)

        if response.status_code not in range(200, 299):
            raise RuntimeError()

        return response.text

    except rq.RequestException:
        lg.error('request error occurred')

    except rq.HTTPError:
        lg.error('connection error occurred')

    except rq.TooManyRedirects:
        lg.error('too many redirects')

    except rq.Timeout:
        lg.error('connection timeout')

    except RuntimeError:
        lg.error('request failed')


def print_metadata(sitename, html):
    links_count = len(html.find_all('a'))
    images_count = len(html.find_all('img'))
    time = datetime.utcnow().strftime('%a %b %d %Y %H:%M UTC')

    print(f'site: {sitename}')
    print(f'num_links: {links_count}')
    print(f'images: {images_count}')
    print(f'last_fetch: {time}')


def archive_asset(session, assets_dir, sitename, html, tag, ref_prop, ext):
    for found_tag in html.find_all(tag):
        ref = found_tag.get(ref_prop)

        if ref and ref.startswith('/') and ref.endswith(ext):
            file_name = Path(ref).name
            file_path = assets_dir.joinpath(file_name)

            if not file_path.exists():
                response = session.get(urljoin(f'https://{sitename}', ref))

                if response.status_code == 200:
                    if ext in ['.png', '.jpg']:
                        with file_path.open('wb') as af:
                            for chunk in response:
                                af.write(chunk)
                    else:
                        file_path.write_text(response.text)

            found_tag[ref_prop] = str(file_path)


def archive_assets(session, sitename, html):
    assets_dir = Path(sitename.replace('.', '_') + '_assets')

    if not assets_dir.exists():
        assets_dir.mkdir()

    archive_asset(session, assets_dir, sitename, html, 'link', 'href', '.css')
    archive_asset(session, assets_dir, sitename, html, 'script', 'src', '.js')
    archive_asset(session, assets_dir, sitename, html, 'img', 'src', '.png')
    archive_asset(session, assets_dir, sitename, html, 'img', 'src', '.jpg')

    return html


def main():
    parser = ArgumentParser()
    parser.add_argument('urls', nargs='+')
    parser.add_argument('-a', '--assets', action='store_true',
                        dest='is_with_assets',
                        help='archive the assets')
    parser.add_argument('-m', '--metadata', action='store_true',
                        dest='is_with_metadata',
                        help='print metadata')
    parser.add_argument('-v', '--version', action='version', version='v1.0.0')
    args = parser.parse_args()

    session = rq.Session()

    for url in args.urls:
        if not re.match(r'https://www\..+', url):
            lg.error(f"can't process url format {url}")
            continue

        html_text = get_html_text(session, url)
        sitename = urlparse(url).netloc

        if html_text:
            html = XMLParser(html_text, 'html.parser')

            if args.is_with_metadata:
                print_metadata(sitename, html)

            if args.is_with_assets:
                html = archive_assets(session, sitename, html)

            Path(sitename + '.html').write_text(str(html))
        else:
            lg.error(f"can't process url {url}")


if __name__ == '__main__':
    main()
